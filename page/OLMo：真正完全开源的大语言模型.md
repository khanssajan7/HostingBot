# OLMo：真正完全开源的大语言模型

在人工智能领域，**完全开源** 的项目并不多见，而由**AI2**（一个真正的非营利性组织）推出的 **OLMo（Open Language Model）** 正是其中之一。OLMo 不仅实现了 **100% 开源**，还开放了其完整的预训练数据、训练代码、模型权重、推理代码、训练指标和完整日志等所有原始数据。

![OLMo 框架](https://bbtdd.com/img/426561011667.webp)

与其他开源语言模型不同，**OLMo** 以其“完全开放的框架”著称，使研究人员能够复现模型训练过程，深入理解模型性能，并根据需求进行微调。

## OLMo 的核心特点

**完全开放的数据和框架** 使 OLMo 成为研究者和开发者的理想选择。以下是 OLMo 框架的主要组成部分：

1. **完整的预训练数据**  
   OLMo 使用的 **Dolma 数据集** 是一个包含 **3 万亿 token** 的开放语料库，涵盖了网页、代码、社交媒体、STEM 论文、书籍等多种数据源。这使得研究人员能够深入理解模型的学习基础，甚至重新训练或调整模型以适应特定需求。

2. **训练代码和模型权重**  
   OLMo 提供了四种不同变体模型的完整模型权重，每种模型都至少训练到 **2 万亿 token**。此外，还包括训练代码、推理代码、训练指标和日志，确保了 **100% 的可复现性**。

3. **评估工具**  
   项目包含了开发过程中使用的评估套件、500 多个模型检查点以及评估代码。这些工具属于 **Catwalk 项目** 的一部分，使研究人员能够评估自己的模型或对 OLMo 进行进一步分析。

## OLMo 的模型参数与架构

OLMo 提供了不同规模的模型变体：

- **1B（10 亿参数）模型**：16 层，每层 2048 个隐藏单元，16 个注意力头，训练了至少 2 万亿 token。
- **7B（70 亿参数）模型**：32 层，每层 4086 个隐藏单元，32 个注意力头，训练了约 2.46 万亿 token。
- **65B（650 亿参数）模型**（训练中）：计划包含 80 层，每层 8192 个隐藏单元，64 个注意力头。

这些模型基于 **解码器仅 Transformer 架构**，并进行了多项改进，例如：

- 不使用偏置项以提高训练稳定性。
- 采用非参数层归一化。
- 使用 SwiGLU 激活函数代替 ReLU。
- 引入旋转位置嵌入（RoPE）。
- 使用修改版的 BPE-based 标记器，以减少个人可识别信息（PII）。

## 性能评估

OLMo 7B 在许多生成和阅读理解任务（如 **truthfulQA**）上与 **Llama 2** 不相上下，但在流行的问答任务（如 **MMLU** 或 **Big-bench Hard**）上略微落后。通过 **Paloma** 评估工具，AI2 分析了模型预测语言的能力与训练 token 数等规模因素之间的关系。

## 项目地址

了解更多关于 OLMo 的信息，请访问：https://allenai.org/olmo

👉 [WildCard | 一分钟注册，轻松订阅海外线上服务](https://bbtdd.com/WildCard)